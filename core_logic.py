import sqlite3
import os
import re
import chardet
import guessit
import time
import hashlib
import json
import numpy as np
from sentence_transformers import SentenceTransformer, util
from openai import OpenAI
import difflib
import sqlite3
import difflib

# --- æ–°å¢è¾…åŠ©å‡½æ•°ï¼šä¸Šä¸‹æ–‡æ‰©å±• ---
def get_context_by_id(db_path, center_id, movie_name, window=1):
    """
    æ ¹æ® ID å‘å‰åæ‰©å±•å°è¯ï¼ŒåŒæ—¶ç¡®ä¿ä¸è·¨è¶Šç”µå½±è¾¹ç•Œã€‚
    """
    try:
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        
        # è¿™é‡Œçš„é€»è¾‘æ˜¯ï¼š
        # 1. ID åœ¨ [center - window, center + window] ä¹‹é—´
        # 2. å¿…é¡»æ˜¯åŒä¸€éƒ¨ç”µå½± (movie_name)
        # 3. æŒ‰ ID æ’åºç¡®ä¿é¡ºåºæ­£ç¡®
        query = """
            SELECT content 
            FROM subtitles 
            WHERE id BETWEEN ? AND ? 
            AND movie_name = ? 
            ORDER BY id ASC
        """
        start_id = center_id - window
        end_id = center_id + window
        
        cursor.execute(query, (start_id, end_id, movie_name))
        results = cursor.fetchall()
        
        # æŠŠç»“æœæ‹¼èµ·æ¥
        combined_text = " ".join([row[0] for row in results])
        return combined_text
        
    except Exception as e:
        print(f"æ‰©å±•ä¸Šä¸‹æ–‡å‡ºé”™: {e}")
        return "" # å‡ºé”™å°±è¿”å›ç©ºï¼Œä¸å½±å“ä¸»æµç¨‹
    finally:
        if conn: conn.close()

DB_NAME = "subtitle_library.db"

# ==========================================
# æ¨¡å— 1ï¼šæ•°æ®åº“åŸºç¡€è®¾æ–½
# ==========================================
def get_db_connection():
    """è·å–æ•°æ®åº“è¿æ¥çš„å®‰å…¨å·¥å‚å‡½æ•°"""
    conn = sqlite3.connect(DB_NAME, check_same_thread=False)
    conn.execute("PRAGMA journal_mode=WAL;") 
    return conn

def init_db():
    conn = get_db_connection()
    c = conn.cursor()
    # --- åˆ›å»ºè¡¨ä¸ç´¢å¼• ---
    c.execute('''
        CREATE TABLE IF NOT EXISTS subtitles (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            file_hash TEXT,
            file_path TEXT,
            movie_name TEXT,
            year INTEGER,
            season INTEGER,   
            episode INTEGER,
            line_index INTEGER,
            start_time TEXT,
            end_time TEXT,
            content TEXT,
            embedding BLOB
        )
    ''')
    # --- Schema è¿ç§»ï¼šè¡¥ç¼ºå¤±åˆ— ---
    c.execute("PRAGMA table_info(subtitles)")
    existing_columns = {row[1] for row in c.fetchall()}
    required_columns = {
        "line_index": "INTEGER",
        "embedding": "BLOB",
        "file_hash": "TEXT",
        "embedding_model": "TEXT",
        "embedding_dim": "INTEGER"
    }
    for col_name, col_type in required_columns.items():
        if col_name not in existing_columns:
            print(f"ğŸ”„ æ£€æµ‹åˆ°æ•°æ®åº“ç¼ºå¤±åˆ— '{col_name}'ï¼Œæ­£åœ¨è‡ªåŠ¨ä¿®è¡¥...")
            try:
                c.execute(f"ALTER TABLE subtitles ADD COLUMN {col_name} {col_type}")
            except Exception as e:
                print(f"âŒ ä¿®è¡¥åˆ— '{col_name}' å¤±è´¥: {e}")
    c.execute('CREATE INDEX IF NOT EXISTS idx_content ON subtitles (content)')
    c.execute('CREATE INDEX IF NOT EXISTS idx_hash ON subtitles (file_hash)')
    c.execute('CREATE INDEX IF NOT EXISTS idx_movie ON subtitles (movie_name)')
    c.execute('CREATE INDEX IF NOT EXISTS idx_context ON subtitles (file_hash, line_index)')
    conn.commit()
    conn.close()

init_db()

def calculate_file_hash(file_bytes):
    return hashlib.md5(file_bytes).hexdigest()

# ==========================================
# æ¨¡å— 2ï¼šè§£æå¼•æ“
# ==========================================
def detect_encoding(file_bytes):
    result = chardet.detect(file_bytes)
    encoding = result['encoding']
    if not encoding or result['confidence'] < 0.5: return 'utf-8-sig'
    if encoding.lower() == 'gb2312': return 'gb18030'
    return encoding

def clean_subtitle_text(text):
    if not text: return ""
    text = re.sub(r'<[^>]+>', '', text) 
    text = re.sub(r'\{[^}]+\}', '', text)
    return text.strip()

def parse_ass_content(content):
    parsed_data = []
    lines = content.split('\n')
    for line in lines:
        if line.startswith('Dialogue:'):
            try:
                parts = line.split(',', 9)
                if len(parts) >= 10:
                    clean_text = parts[9].replace(r'\N', ' ').replace(r'\n', ' ')
                    clean_text = clean_subtitle_text(clean_text)
                    if clean_text:
                        parsed_data.append({'start': parts[1].strip(), 'end': parts[2].strip(), 'text': clean_text})
            except: continue
    return parsed_data

def parse_srt_content(content):
    content = content.replace('\r\n', '\n').replace('\r', '\n')
    # --- æ–¹æ¡ˆ Aï¼šæ ‡å‡† SRT æ­£åˆ™ ---
    pattern_std = re.compile(
        r'\d+\s*\n'
        r'(\d{1,2}:\d{2}:\d{2}[.,]\d{1,3})\s*-->\s*(\d{1,2}:\d{2}:\d{2}[.,]\d{1,3})'
        r'\s*\n([\s\S]*?)(?=\n\n|\n\d+\s*\n|\Z)',
        re.MULTILINE
    )
    matches = pattern_std.findall(content)
    if len(matches) > 0:
        parsed_data = []
        for m in matches:
            clean_text = clean_subtitle_text(m[2]).replace('\n', ' ')
            if clean_text:
                parsed_data.append({
                    'start': m[0].replace(',', '.'), 
                    'end': m[1].replace(',', '.'), 
                    'text': clean_text
                })
        return parsed_data
    # --- æ–¹æ¡ˆ Bï¼šå¤‡ç”¨å…¼å®¹ï¼ˆé‡ç”Ÿæ ¼å¼ï¼‰---
    print("âš ï¸ æ ‡å‡†è§£æå¤±è´¥ï¼Œå¯ç”¨å¤‡ç”¨å…¼å®¹æ¨¡å¼...")
    parsed_data = []
    pattern_fallback = re.compile(
        r'(\d{1,2}:\d{2}:\d{2}[.,]\d{1,3})\s*-->\s*(\d{1,2}:\d{2}:\d{2}[.,]\d{1,3})'
        r'(.*)'
    )
    lines = content.split('\n')
    current_entry = None
    for line in lines:
        line = line.strip()
        if not line:
            continue
        match = pattern_fallback.search(line)
        if match:
            if current_entry:
                parsed_data.append(current_entry)
            start, end, inline_text = match.groups()
            text_content = inline_text.strip()
            text_content = re.sub(r'\[.*?\]', '', text_content).strip()
            current_entry = {'start': start.replace(',', '.'), 'end': end.replace(',', '.'), 'text': text_content}
        else:
            if (
                current_entry and 
                not line.isdigit() and 
                not line.startswith('<') and 
                not line.startswith('[')
            ):
                current_entry['text'] = (current_entry['text'] + " " + line).strip()
    if current_entry:
        parsed_data.append(current_entry)
    return parsed_data

def parse_vtt_content(content):
    return parse_srt_content(content.replace('WEBVTT', ''))

# ==========================================
# æ¨¡å— 3ï¼šä¸šåŠ¡å¤„ç†ï¼ˆæ–‡ä»¶åè§£æã€æ‰¹é‡å¤„ç†ä¸å…¥åº“ï¼‰
# ==========================================
def analyze_filenames(uploaded_files):
    results = []
    for file in uploaded_files:
        info = guessit.guessit(file.name)
        title = info.get('title')
        if not title or title == 'æœªçŸ¥ç”µå½±' or '[' in str(title):
            anime_match = re.search(r'^\[.*?\]\[(.*?)\]', file.name)
            if anime_match: title = anime_match.group(1).replace('_', ' ').strip()
            else:
                movie_match = re.search(r'^\[(.*?)\]', file.name)
                if movie_match: title = movie_match.group(1).strip()
        if not title: title = file.name
        s_num = info.get('season', 0)
        e_num = info.get('episode', 0)
        results.append({
            "åŸå§‹æ–‡ä»¶å": file.name,
            "è¯†åˆ«ç‰‡å": title,
            "å¹´ä»½": info.get('year', 0),
            "season_num": s_num,
            "episode_num": e_num,
            "å‰§é›†": f"S{str(s_num).zfill(2)}E{str(e_num).zfill(2)}" if e_num else "",
            "çŠ¶æ€": "å¾…ç¡®è®¤"
        })
    return results

# --- æ‰¹é‡å¤„ç†ï¼šè§£æã€è½ç›˜ã€å¾…å…¥åº“ ---
def _process_one_batch(file_objects, metadata_list, target_folder, conn, model, model_name=""):
    logs = []
    processed_files = []
    pending_rows = []
    stats = {"success": 0, "fail": 0, "duplicate": 0}
    c = conn.cursor()
    for idx, (file_obj, meta) in enumerate(zip(file_objects, metadata_list)):
        try:
            file_obj.seek(0)
            raw_bytes = file_obj.read()
        except Exception as e:
            logs.append(f"âŒ è¯»å–å¤±è´¥: {getattr(file_obj, 'name', 'file')} - {e}")
            stats["fail"] += 1
            continue
        f_hash = calculate_file_hash(raw_bytes)
        c.execute("SELECT movie_name FROM subtitles WHERE file_hash = ?", (f_hash,))
        if c.fetchone():
            logs.append(f"âš ï¸ è·³è¿‡é‡å¤: {getattr(file_obj, 'name', 'file')}")
            stats["duplicate"] += 1
            continue
        try:
            encoding = detect_encoding(raw_bytes)
            content = raw_bytes.decode(encoding, errors='replace')
        except Exception as e:
            logs.append(f"âŒ è§£ç å¤±è´¥: {getattr(file_obj, 'name', 'file')} - {e}")
            stats["fail"] += 1
            continue
        ext = (getattr(file_obj, 'name', '') or '').split('.')[-1].lower()
        if '[Script Info]' in content[:1000] and 'Dialogue:' in content:
            subs = parse_ass_content(content)
        elif ext in ['ass', 'ssa']:
            subs = parse_ass_content(content)
        elif ext == 'vtt':
            subs = parse_vtt_content(content)
        else:
            subs = parse_srt_content(content)
        if not subs:
            logs.append(f"âš ï¸ æ— æœ‰æ•ˆå­—å¹•: {getattr(file_obj, 'name', 'file')}")
            stats["fail"] += 1
            continue
        embeddings = []
        if model:
            try:
                texts = [s['text'] for s in subs]
                if texts:
                    embeddings = model.encode(texts, convert_to_numpy=True, show_progress_bar=False)
            except Exception as e:
                logs.append(f"âš ï¸ å‘é‡åŒ–è®¡ç®—å¤±è´¥: {e}")
        clean_title = str(meta.get('è¯†åˆ«ç‰‡å', '')).replace('/', '_').replace(':', ' ')
        new_name = f"{clean_title}"
        if meta.get('å¹´ä»½'): new_name += f" ({meta['å¹´ä»½']})"
        if meta.get('season_num') or meta.get('episode_num'):
            new_name += f".S{str(meta.get('season_num', 0)).zfill(2)}E{str(meta.get('episode_num', 0)).zfill(2)}"
        new_name += ".srt"
        save_path = os.path.join(target_folder, new_name)
        try:
            with open(save_path, 'w', encoding='utf-8') as f_out:
                f_out.write(content)
        except Exception as e:
            logs.append(f"âŒ å†™å…¥å¤±è´¥: {new_name} - {e}")
            stats["fail"] += 1
            continue
        current_dim = 0
        if model and len(embeddings) > 0:
            current_dim = embeddings.shape[1]
        processed_files.append({"name": new_name, "content": content})
        for i, s in enumerate(subs):
            emb_blob = embeddings[i].tobytes() if (model and i < len(embeddings)) else None
            row = (f_hash, save_path, meta.get('è¯†åˆ«ç‰‡å'), meta.get('å¹´ä»½'),
                   meta.get('season_num'), meta.get('episode_num'),
                   i, s['start'], s['end'], s['text'], emb_blob,
                   model_name if model else None,
                   current_dim if model else 0
                   )
            pending_rows.append(row)
        status_tag = "(å«ç´¢å¼•)" if model else "(æé€Ÿ/æ— ç´¢å¼•)"
        logs.append(f"âœ… å¤„ç†å®Œæˆ {status_tag}: {new_name}")
        stats["success"] += 1
    return logs, processed_files, stats, pending_rows

def process_only(file_objects, metadata_list, target_folder, model, model_name=""):
    if not os.path.exists(target_folder):
        os.makedirs(target_folder)
    conn = get_db_connection()
    try:
        return _process_one_batch(file_objects, metadata_list, target_folder, conn, model, model_name)
    finally:
        conn.close()

def commit_pending_to_db(pending_rows):
    if not pending_rows: return
    conn = get_db_connection()
    try:
        c = conn.cursor()
        c.executemany('INSERT INTO subtitles VALUES (NULL,?,?,?,?,?,?,?,?,?,?,?,?,?)', pending_rows)
        conn.commit()
    finally:
        conn.close()


# ==========================================
# æ¨¡å— 4ï¼šç»Ÿè®¡ä¸å…¨é‡æ‰«æ
# ==========================================
def get_library_stats():
    conn = get_db_connection()
    c = conn.cursor()
    c.execute("SELECT COUNT(DISTINCT movie_name) FROM subtitles")
    m_count = c.fetchone()[0]
    c.execute("SELECT COUNT(*) FROM subtitles")
    l_count = c.fetchone()[0]
    conn.close()
    return {"movie_count": m_count, "line_count": l_count, "last_update": time.strftime("%Y-%m-%d %H:%M:%S")}

def scan_library_path(library_path, model, model_name=""):
    """æ‰«æç›®å½•å¹¶å‘é‡åŒ–å…¥åº“ï¼Œä½¿ç”¨ä¼ å…¥çš„æ¨¡å‹å¯¹è±¡ã€‚"""
    if not os.path.exists(library_path): 
        yield "âŒ é”™è¯¯: è·¯å¾„ä¸å­˜åœ¨", None
        return
    if not model:
        yield "âŒ æ¨¡å‹åŠ è½½å¤±è´¥", None
        return
    conn = get_db_connection()
    c = conn.cursor()
    c.execute("SELECT file_path FROM subtitles")
    db_files = {os.path.normpath(row[0]) for row in c.fetchall()}
    new_count = 0
    valid_exts = ('.srt', '.txt', '.ass', '.ssa', '.vtt')
    disk_files = set()
    for root, dirs, files in os.walk(library_path):
        for file in files:
            if file.lower().endswith(valid_exts):
                full_path = os.path.normpath(os.path.join(root, file))
                disk_files.add(full_path)
                if full_path not in db_files:
                    try:
                        with open(full_path, 'rb') as f: raw_bytes = f.read()
                        f_hash = calculate_file_hash(raw_bytes)
                        encoding = detect_encoding(raw_bytes)
                        content = raw_bytes.decode(encoding, errors='replace')
                        ext = file.lower().split('.')[-1]
                        if ext == 'txt': subs = [{'start':'0','end':'0','text':l.strip()} for l in content.split('\n') if l.strip()]
                        elif ext in ['ass','ssa']: subs = parse_ass_content(content)
                        elif ext == 'vtt': subs = parse_vtt_content(content)
                        else: subs = parse_srt_content(content)
                        if not subs: continue
                        texts = [s['text'] for s in subs]
                        embeddings = []
                        current_dim = 0
                        if texts:
                            embeddings = model.encode(texts, convert_to_numpy=True, show_progress_bar=False)
                            if len(embeddings) > 0: current_dim = embeddings.shape[1]
                        info = guessit.guessit(file)
                        title = info.get('title') or file
                        s_num = info.get('season', 0)
                        e_num = info.get('episode', 0)
                        data_to_insert = []
                        for i, s in enumerate(subs):
                            emb_blob = embeddings[i].tobytes() if i < len(embeddings) else None
                            data_to_insert.append((
                                f_hash, full_path, title, info.get('year', 0), s_num, e_num,
                                i, s['start'], s['end'], s['text'], emb_blob,
                                model_name, current_dim
                            ))
                        c.executemany('INSERT INTO subtitles VALUES (NULL,?,?,?,?,?,?,?,?,?,?,?,?,?)', data_to_insert)
                        new_count += 1
                        yield f"âœ… å·²å‘é‡åŒ–å…¥åº“: {file}", new_count
                    except Exception as e:
                        yield f"âŒ å¤±è´¥ {file}: {e}", new_count
    conn.commit()
    missing = list(db_files - disk_files)
    conn.close()
    yield "DONE", {"success": True, "new_added": new_count, "missing_files": missing}

def delete_missing_records(paths):
    conn = get_db_connection()
    c = conn.cursor()
    for p in paths: c.execute("DELETE FROM subtitles WHERE file_path = ?", (p,))
    conn.commit()
    conn.close()
    return True, "æ¸…ç†æˆåŠŸ"

# ==========================================
# æ¨¡å— 5ï¼šæ£€ç´¢ä¸ AI æ¥å£
# ==========================================
def get_all_movies():
    """è¿”å›å·²å…¥åº“ç”µå½±ååˆ—è¡¨ã€‚"""
    conn = get_db_connection()
    c = conn.cursor()
    c.execute("SELECT DISTINCT movie_name FROM subtitles ORDER BY movie_name")
    movies = [row[0] for row in c.fetchall()]
    conn.close()
    return movies

def search_db_keyword(keyword):
    conn = get_db_connection()
    c = conn.cursor()
    c.execute("SELECT movie_name, season, episode, start_time, content FROM subtitles WHERE content LIKE ? LIMIT 50", (f'%{keyword}%',))
    rows = c.fetchall()
    conn.close()
    return [{"movie": r[0], "season": r[1], "episode": r[2], "time": r[3], "content": r[4]} for r in rows]

def get_context_lines(movie_name, season, episode, line_index, window=2):
    conn = get_db_connection()
    c = conn.cursor()
    query = '''
        SELECT start_time, content, line_index 
        FROM subtitles 
        WHERE movie_name = ? AND season = ? AND episode = ?
          AND line_index BETWEEN ? AND ?
        ORDER BY line_index ASC
    '''
    start_idx = max(0, line_index - window)
    end_idx = line_index + window
    c.execute(query, (movie_name, season, episode, start_idx, end_idx))
    rows = c.fetchall()
    conn.close()
    return [{"time": r[0], "text": r[1], "is_target": (r[2] == line_index)} for r in rows]

def search_db_semantic(query, model, top_k=20, db_path=DB_NAME, target_movie=None):
    """
    åŸºç¡€è¯­ä¹‰æœç´¢ï¼šæ”¯æŒæŒ‰ç”µå½±åè¿‡æ»¤
    """
    # å°†æœç´¢è¯å‘é‡åŒ–
    query_vector = model.encode(query)
    
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    
    try:
        # âœ… æ­£ç¡®çš„ SQL é€»è¾‘ï¼ˆä½¿ç”¨æ ‡å‡†åŠè§’ç©ºæ ¼ï¼‰
        if target_movie:
            query_sql = "SELECT id, movie_name, season, episode, start_time, content, embedding FROM subtitles WHERE movie_name = ?"
            cursor.execute(query_sql, (target_movie,))
        else:
            query_sql = "SELECT id, movie_name, season, episode, start_time, content, embedding FROM subtitles"
            cursor.execute(query_sql)
            
        rows = cursor.fetchall()
        
        import numpy as np
        q_vec = np.array(query_vector)
        candidates = []
        
        for row in rows:
            # âœ… æ­£ç¡®çš„äºŒè¿›åˆ¶è¯»å–é€»è¾‘
            embedding_blob = row[6]
            if not embedding_blob:
                continue
            
            db_vec = np.frombuffer(embedding_blob, dtype=np.float32)
            
            # é˜²å¾¡æ€§æ£€æŸ¥ï¼šç»´åº¦å¿…é¡»åŒ¹é…æ‰èƒ½è®¡ç®—
            if db_vec.shape != q_vec.shape:
                continue
                
            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            score = np.dot(q_vec, db_vec) / (np.linalg.norm(q_vec) * np.linalg.norm(db_vec))
            
            candidates.append({
                'id': row[0],
                'movie': row[1],
                'season': row[2],
                'episode': row[3],
                'time': row[4],
                'content': row[5],
                'score': score
            })
            
        # æ’åºå¹¶å– Top K
        candidates.sort(key=lambda x: x['score'], reverse=True)
        return candidates[:top_k]

    finally:
        conn.close()

# --- LLM è°ƒç”¨ ---
def call_llm_api(system_prompt, user_prompt, api_key, model_name, base_url):
    """
    é€šç”¨å¤§æ¨¡å‹è°ƒç”¨æ¥å£ï¼Œé€‚é…æ‰€æœ‰ OpenAI æ ¼å¼çš„ APIã€‚
    """
    if not api_key and "ollama" not in base_url.lower():
        raise ValueError("æœªé…ç½® API Keyï¼Œè¯·åœ¨ä¾§è¾¹æ è®¾ç½®")
    
    try:
        # è¿™é‡Œçš„ client ä¼šè‡ªåŠ¨é€‚é…ä½ å¡«å†™çš„å‚å•† URL
        client = OpenAI(api_key=api_key, base_url=base_url)
        
        response = client.chat.completions.create(
            model=model_name, # ä½¿ç”¨ UI é€‰ä¸­çš„æ¨¡å‹
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            stream=False,
            temperature=0.7,
            max_tokens=4000
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"âŒ AI è°ƒç”¨å¤±è´¥ï¼š{str(e)}"

# ==================================================
# æ–°å¢åŠŸèƒ½ï¼šä¼˜åŒ–ç‰ˆè¯­ä¹‰æœç´¢ (å¸¦å»é‡å’Œæ¼æ–—è¿‡æ»¤)
# ==================================================


def search_db_semantic_optimized(query, model, db_path, final_k=20, fetch_ratio=4, allow_duplicates=False, target_movie=None):
    """
    å·²é›†æˆä¸Šä¸‹æ–‡æ‰©å±•åŠŸèƒ½
    æ³¨æ„ï¼šå¢åŠ äº†ä¸€ä¸ª db_path å‚æ•°ï¼Œå› ä¸ºæ‰©å±•å‡½æ•°éœ€è¦è¿æ•°æ®åº“
    """
    # 1. å®½è¿›
    fetch_k = final_k * fetch_ratio
    # è¿™é‡Œçš„è°ƒç”¨è¦ç¡®ä¿ä¼ å¯¹å‚æ•°
    raw_results = search_db_semantic(query, model, top_k=fetch_k, db_path=db_path, target_movie=target_movie)
    
    if not raw_results:
        return []

    unique_results = []
    seen_contents = []

    for res in raw_results:
        # --- æ ¸å¿ƒä¿®æ”¹ï¼šä¸Šä¸‹æ–‡æ‰©å±• (Magic Happens Here) ---
        # å¦‚æœä¸æ˜¯æ··å‰ªæ¨¡å¼ï¼ˆå³å†™å‰§æœ¬æ¨¡å¼ï¼‰ï¼Œæˆ‘ä»¬è¿›è¡Œæ‰©å±•
        if not allow_duplicates:
            # å‘å‰åå„æ‰©å…… 1 å¥ (window=1)
            expanded_text = get_context_by_id(db_path, res['id'], res['movie'], window=1)
            
            # å¦‚æœæ‰©å±•æˆåŠŸï¼Œå°±ç”¨æ‰©å±•åçš„æ–‡æœ¬æ›¿æ¢åŸæ¥çš„å•å¥
            if expanded_text:
                res['content'] = expanded_text 
                # å°æŠ€å·§ï¼šå¯ä»¥åœ¨æ˜¾ç¤ºæ—¶åŠ ä¸ªæ ‡è®°ï¼Œå‘Šè¯‰ç”¨æˆ·è¿™æ˜¯æ‰©å±•è¿‡çš„
                # res['content'] = f"{expanded_text} (AIè”æƒ³)"

        # --- ä¸‹é¢æ˜¯åŸæ¥çš„å»é‡é€»è¾‘ ---
        new_text = res['content']

        if allow_duplicates:
            is_same_source = False
            for exist in unique_results:
                if exist['movie'] == res['movie'] and exist['time'] == res['time']:
                    is_same_source = True
                    break
            if not is_same_source:
                unique_results.append(res)
            if len(unique_results) >= final_k: break
            continue 

        # ä¸¥æ ¼å»é‡é€»è¾‘
        if len(new_text) < 2: continue
        if new_text in seen_contents: continue
        
        is_duplicate = False
        for seen in seen_contents:
            similarity = difflib.SequenceMatcher(None, new_text, seen).ratio()
            # å› ä¸ºæ‰©å±•åæ–‡æœ¬å˜é•¿äº†ï¼Œç›¸ä¼¼åº¦é˜ˆå€¼å¯ä»¥ç¨å¾®è°ƒä½ä¸€ç‚¹ï¼Œæˆ–è€…ä¿æŒä¸å˜
            if similarity > 0.85: 
                is_duplicate = True
                break
        
        if is_duplicate: continue

        unique_results.append(res)
        seen_contents.append(new_text)
        
        if len(unique_results) >= final_k:
            break
            
    return unique_results

    
def generate_script(movie_name, api_key):
    """è°ƒç”¨ LLM ç”Ÿæˆæ··å‰ªè„šæœ¬ã€‚"""
    sys_prompt = "ä½ æ˜¯ä¸€ä½ç”µå½±é¢„å‘Šç‰‡å‰ªè¾‘å¤§å¸ˆ..."
    usr_prompt = f"ç”µå½±åç§°ï¼šã€Š{movie_name}ã€‹..."
    return call_deepseek_llm(sys_prompt, usr_prompt, api_key)

def extract_golden_quotes(subtitle_text, api_key):
    """é‡‘å¥æç‚¼ï¼ˆé¢„ç•™ï¼‰ã€‚"""
    sys_prompt = "ä½ æ˜¯ä¸€ä¸ªæ–‡å­¦é‰´èµå®¶ã€‚è¯·ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æ‰¾å‡º 5 å¥æœ€å¯Œæœ‰å“²ç†çš„é‡‘å¥ã€‚"
    usr_prompt = subtitle_text[:5000]
    return call_deepseek_llm(sys_prompt, usr_prompt, api_key)

